This project is going to be a command line application that runs on our local machines using Node.js. The purpose of this application is to crawl a 
website and produce a report of the internal linking structure of the website. Basically which pages link to which other pages on the site. This is a
tool that would be traditionally used by web marketers or SEO (Search Engine Optimisation) experts.
1. First we create a new repo in github for the project that we are starting. Next, we clone that repository inside a folder where we want to create
the app using the command "git clone <repo link>"
2. Next, we install NVM or the node version. After that we will use the command "nvm install <version>" followed by the command "nvm use <version>"
3. Next, to check whether everything is working, we will create a main.js file and console.log "Hello World". To run it, we will type, "node main.js"
and this starts the node interpreter and passes in main.js as the entry point. It logs "Hello World" and we are good to go. 
4. Running the node kind of manually from the command line, like the node and then the name of a file, it works just fine. But, because we're going to
be installing a test suite and packages, we're actually going to use NPM to manage all that environment and ecosystem. So, inorder to get that going, 
we just run the command "npm init" and press enter to set all the values as default. This will create a package.json file, in this file the only thing
that we need to change at this point is that we need to add a new script that we'll just call "start" that runs "node main.js" for us. So, instead of
manually typing node main.js into the command line, we will type npm start and that will call the script that we just wrote now. The purpose of this is
now that we're using a package.json file, when we pull in dependencies and run our code through npm, the dependencies will be available to our code. 
Similarly, we will be using npm to run our tests so its convenient to have everything in the start command. We run the command once to check if its
working.
5. So, we're going to be doing some test-driven development as we build out this project and in order to do so we need a test suite. So, we're going to
install Jest a fairly popular testing runtime. We can do that by typing, "npm install --save-dev jest". This will creat the package-lock.json file
and the node_modules folder.
6. Now, we don't want these to be uploaded to our repo so we add a ".gitignore" file and add the node_modules to it. We do this because this folder
contains all of the dependencies for our project and we don't want to commit that code to our repository, we just want the code that is unique to
our application. So, we'll keep track of our dependencies in the package.json, which is committed to our source control. So, anytime we pull down this 
repository fresh, it won't have the node modules so a developer will just need to type, 'npm install' in order to install all the dependencies listed
in the package.json.
7. Now, that we have Jest added as a dependency, we will add a script to actually run it. We will swap out the echo command in the test script in
package.json with "jest". Now, if we run 'npm test' it will run and say no tests found so now we are properly calling Jest and now we just need to
add some tests.
8. Now, the first function that we write tests for is going to be called normalize URLs and we will create a file called crawl.js to write that
function in. Usually when we are doing test-driven development, there are three steps, the first step is to stub out the function that we want to test,
the second step is to write the tests for the function and the third step is to go back and actually implement the main crux of the function. Following
this, we will first stub out our function what this really means is that the function does not do anything useful as such but it gives us an idea of 
what goes in and what comes out. Next, we write the test file called crawl.test.js and the way that jest works is that it just automatically looks for
files in your package or in your folder project directory that end in .test.js so it is important that we name it with that suffix. Heading back to our
crawl.js file we will add a module.exports which will make the normalizeURL function available to other JS files that want to import it. Now, we go back
to our test file and import the function as well as two more functions from jest called 'test' and 'expect'. So, the way testing works with Jest is that 
there is a top test function and it takes as input the name of the test that will be the name of our function that we made, normalizeURL. The second 
input is a function and within this function we can use the expect function to do a test. To do that we will first specify what will be the input that 
we will give to the normalizeURL function as the variable 'input' next we will pass the input to the function and save it as the variable 'actual' in 
the sense that this will be the actual output of the input given to the function and then we will define a variable expected that will hold the value 
that we expect the function to give when the specified input is passed to the function. Then we will use the expect function, pass it the variable 
'actual' and use the method 'toEqual' with it and pass it the variable 'expected'. This line says that we are expecting the actual output of normalizeURL 
to equal the expected output that we've specified.If they do equal each other, Jest will log it as a pass test. If they don't equal each other, then jest 
is going to say the test failed. To run the tests we can simply type the command npm test.
9. Now, that our testing environment is ready we will now actually implement the normalizeURL function. Sometimes on the Internet, there are different
URLs that all point to effectively the same page. So 'https://google.com', 'http://google.com' and 'https://Google.com' are all basically URL strings
for the same webpage. So basically, if all these things are put into the normalizeURL function, we want the same thing to come out. Now, we will build
the actual test suite. We will take any of the above URL strings as an input and the first thing that we need to do is strip down the protocol. The 
domain name and the path together make up the address of the webpage. We don't care about query parameters, protocols etc. when it comes to what page 
does this URL represent. And thats how we write our test too, we set the output as the URL that does not contain any of the above. Now, we need to write 
a function to do that to our string. We implemented this function using the built-in URL constructor. This will give us a URL object with a bunch of
properties like hostname and pathname and that's what we are going to return both as a string. Next thing that we want our normalizeURL function to 
handle is trailing slashes. So, we will add a new test to our test suite and in this test as an input we will add a trailing slash on the URL and we
will deal with it in our function as well. Next test we will write is to remove capitals from our URL and normalize them to lowercase. We don't need
to write any extra logic for this because the URL constructor is doing it for us bcoz it knows that URLs are case insensitive.
10. Next, we are going to add another function called get URLs from HTML and in order to be able to write that function we are going to need a new 
dependency i.e. JS DOM. The interesting thing about this dependency is that it is not a dev dependency. Dev dependencies are just intended to be used
by the developer whereas dependencies are intended to be used by the application when it's running in production. Our getURLsFromHTML function is going
to take two inputs first will be a string representing some HTML called htmlBody and the second input will be the baseURL which represents the URL of the
website that we are crawling. This function returns an array of strings. We first write a stub of this function, passing the inputs as mentioned above
and defining an empty array and returning it as it is. Remember to export this new function that we are creating as well and import it in our test suite
as well. Now, we will create some tests for this function as well in this, we need to pass two inputs, one a body of an HTML and the other a base URL
of the website. The baseURL represents the URL of the website that we are crawling and htmlBody is the body part of the HTML of our site. Remember to 
pass the right inputs to the right function. Now, the purpose of the getURLsFromHTML function is to grab all of the URLs or links embedded within an HTML 
page. So, when we visit a web page, and there's tons of clickable links, we want to grab all of those clickable links out of the document and return 
them in an array of strings. That is what we will write in our function. Don't forget to export and import the functions. Now we will add some tests for
this. We will be using the JSDOM package so we will require it in our crawl.js. The way that JS DOM works is that it basically gives us a way to access 
DOM APIs or Document Object Model APIs. So, we create this DOM object by calling new JS DOM and passing it some HTML like htmlBODY. So, its taking that
HTML as a string and creating a DOM, basically an in-memory object that represents that HTML tree structure. dom.window.document.querySelectorAll('a');
This will return us link elements. This syntax is what we would use in the browser to work with the DOM. The package JS DOM is allowing us to do that in
node so we can write a command line script that interacts with the DOM and crawls the web pages. The reason we are passing 'a' in it is because we want a
list of all the 'a' tags in the HTML. Next we loop over that array and the linkElements is an object that has a property and the property we're interested
in is the href property so we push this into the urls array. Adding another test, we are not just pulling out absolute URLs we are also pulling out relative
URLs. A relative URL is a URL that does not include the protocol and the domain, it just includes a path. To the browser, the relative URL is the same
as the absolute URL because the way that the browser works is it looks at what domain you are on when you are viewing a page and all of the links in the
page if they are relative, it will assume that they are kind of attached to the same domain. So, the actual URL would be the baseURL + the relative URL.
Now, we don't want to extract the relative URL out of the HTML instead we want to combine it into a full absolute URL so we can actually make requests
to it later. Basically it means that we want to pull out the full, complete URL. To write the function, we will first break down the logic tree based on
whether we're working with absolute or relative URLs. Next, we will add a test that will pull out multiple links of both relative and absolute kinds and
we won't need to change our function for this test to pass. One last test for this function is that when we are given a bad URL, we don't include it in
the extracted URLs. To do that we will use the URL object constructor and pass it the linkElements, if the URL object is made then the URL will be pushed
and if the URL object is not made then it will throw an error and we will console log it and the invalid URL will not be extracted.
11. So far, we have been writing helper functions and written some tests to make sure that they are working. We should write a main function which will
be the entry point to the application and all it should do at this point is process command line arguments because we are going to provide the link of the
website that we want to crawl, through the command line. In node, this is pretty easy. There's an object at the global level called process and argv is a 
property that we can use to grab the command line arguments. Within the main function, we first check to make sure that the command line arguments have
been passed in properly. For that we check the length of the argv and we want it to be greater than 2. The reason behind this is that if we give our
command line no argument, the lenght of the array is 2. The first argument is the interpreter, its strange but the first argument to any program is
always the name of the program, that's how these operating systems work. Node is technically the name of our program because node is what's running and
interpretting our code because we're using an interpreted programming language. The second argument to our program is the name of our code or the entry
point file. If we actually pass in an argument to our program, that argument will be the third argument. Hence, the length of argv must be greater than
or equal to 3. To make our checking more robust so that not more than one website is provided we will add another check for too many arguments for >3.
Once all these checks are passed, we will be sure that we have only 3 arguments and we will be able to pick out our base URL to be the 2nd element.
12. Now, we deal with the HTTP part of the project. We go to the crawl.js file and create a new function called crawlPage and as input it will take the 
currentURL. Next, we will make a fetch request to that URL, this is going to be a GET request because we're hoping to get back some HTML. We don't even 
need to specify any options, fetch gets requests by default. Now, we are expecting the response body to be formatted as HTML instead of JSON so we're 
going to just parse it as text again, because we already wrote getURLsFromHTML which takes HTML as a string, as text, as input. So, we can just grab it 
as text. Remember to export and import crawl.js file in the main.js file. Another thing that we need to take care of is that if the user types in some
invalid URL, the response is the stack trace of the error and this is not a good UX. We should give prettier error messages and exit cleanly when there's
bad user input but also as we crawl pages, because we're going to be crawling multiple pages with our tool, we don't want to encounter one bad link and
have the whole process just kind of vomit and exit, we want to be able to handle that and keep crawling and moving. For that we are going to add some
try-catch handling. So, we have checked for an error in the fetch call but we also want to check some more things because there are some other things
that could go wrong. The first thing is the status code, we need to check and ensure that the status code of this fetch request is a 200 to 300 level code.
Another thing that we need to check for is that we are actually getting HTML back i.e. when we parse the body, does it actually contain text that represents
valid HTML. The way that we are going to do that is by parsing the headers.